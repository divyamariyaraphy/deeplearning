{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLSECONDDAY.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1aqjfFetjLg",
        "colab_type": "code",
        "outputId": "38f5a562-d6c8-42bd-a357-678f8e74a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#MLP\n",
        "import numpy as np\n",
        "# Load the dataset from sklearn\n",
        "from sklearn.datasets  import load_breast_cancer\n",
        "# Use the MLP class defined in sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# Load the dataset\n",
        "cancerDataset  =load_breast_cancer()\n",
        "# Print a detailed description of the dataset\n",
        "print(cancerDataset.DESCR)\n",
        "# Load the attributes and target in X and y\n",
        "X = cancerDataset.data\n",
        "y =cancerDataset.target\n",
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2 ,stratify=y,\n",
        "random_state=42)\n",
        "# Create an object (model) for MLP.\n",
        "mlpClassifier=MLPClassifier(solver='lbfgs', alpha= 1e-5 , hidden_layer_sizes=( 5  ,  2  ),\n",
        "random_state= 1)\n",
        "mlpClassifier.fit(X_train, y_train)\n",
        "score  =  mlpClassifier.score(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry \n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
            "        13 is Radius SE, field 23 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n",
            "0.3684210526315789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsf4ggz3eSQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#random forest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5SnB3r8esda",
        "colab_type": "code",
        "outputId": "fab8331c-d8bf-4232-f6e5-ffe4a14bb9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import numpy as np\n",
        "# Load the dataset from sklearn\n",
        "from sklearn.datasets import fetch_covtype\n",
        "# Use the MLP class defined in sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# When return_X_y = True, the load function\n",
        "# return data and target instead of Bunch object.\n",
        "X, y = fetch_covtype(return_X_y=True)\n",
        "\n",
        "\n",
        "print(type(X))\n",
        "# <class 'numpy.ndarray'>\n",
        "\n",
        "print(X.shape)\n",
        "# (581012, 54)\n",
        "print(y.shape)\n",
        "# (581012,)\n",
        "# Reduce the number of attributes, consider only first 10 attributes.\n",
        "X_10 = X[:,:10]\n",
        "\n",
        "print(X_10.shape)\n",
        "# (581012, 10)\n",
        "# Split the data into 90% training and 10% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# The 10% testing data obtained during this split will be take as our entire database.\n",
        "# This is because the original dataset is too big.\n",
        "X10_train, X10_test, y10_train, y10_test = train_test_split(X_10, y, test_size=0.1,\n",
        "stratify=y, random_state=42)\n",
        "\n",
        "print(X10_test.shape)\n",
        "# (58102, 10)\n",
        "\n",
        "# Handle only the modified 1% dataset. Split that into training and testing.\n",
        "# X and y are updated with the downsized dataset\n",
        "X = X10_test\n",
        "y = y10_test\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "stratify=y, random_state=42)\n",
        "\n",
        "# Feature scaling using Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "# Scale both trainign and testing data\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Create an object (model) for MLP.\n",
        "mlpClassifier = MLPClassifier(solver='lbfgs', alpha=1e-6,\n",
        "hidden_layer_sizes=(50, 25), random_state=1)\n",
        "\n",
        "# Train the MLP using 80% training set\n",
        "mlpClassifier.fit(X_train_std, y_train)\n",
        "# MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
        "# beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "# hidden_layer_sizes=(50, 25), learning_rate='constant',\n",
        "# learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
        "# n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
        "# random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
        "# validation_fraction=0.1, verbose=False, warm_start=False)\n",
        "\n",
        "# The classification score accuracy obtained.\n",
        "score = mlpClassifier.score(X_test_std, y_test)\n",
        "print(score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(581012, 54)\n",
            "(581012,)\n",
            "(581012, 10)\n",
            "(58102, 10)\n",
            "0.7542380173823251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5h9XDmfIPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#WINE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF4zKHA2DXPH",
        "colab_type": "code",
        "outputId": "97ecf90e-0398-4327-8b7f-91b88ae94f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "# Load the dataset from sklearn\n",
        "from sklearn.datasets import load_wine\n",
        "# Use the MLP class defined in sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# When return_X_y = True, the load function\n",
        "# return data and target instead of Bunch object.\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "print(type(X))\n",
        "# <class 'numpy.ndarray'>\n",
        "print(X.shape)\n",
        "# (178, 13)\n",
        "print(y.shape)\n",
        "# (178,)\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "stratify=y, random_state=42)\n",
        "# Create an object (model) for MLP.\n",
        "mlpClassifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "hidden_layer_sizes=(25, 25), random_state=1)\n",
        "# Init signature: MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
        "#solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant',\n",
        "#learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,\n",
        "#random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
        "#nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1,\n",
        "#beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "# Docstring:\n",
        "# Multi-layer Perceptron classifier.\n",
        "# This model optimizes the log-loss function using LBFGS or stochastic\n",
        "# gradient descent.\n",
        "# Parameters\n",
        "# ----------\n",
        "# hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
        "# The ith element represents the number of neurons in the ith\n",
        "# hidden layer.\n",
        "\n",
        "\n",
        "# solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
        "# The solver for weight optimization.\n",
        "# - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
        "# - 'sgd' refers to stochastic gradient descent.\n",
        "# - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
        "# by Kingma, Diederik, and Jimmy Ba\n",
        "# Note: The default solver 'adam' works pretty well on relatively\n",
        "# large datasets (with thousands of training samples or more) in terms of\n",
        "# both training time and validation score.\n",
        "# For small datasets, however, 'lbfgs' can converge faster and perform\n",
        "# better.\n",
        "# alpha : float, optional, default 0.0001\n",
        "# L2 penalty (regularization term) parameter.\n",
        "\n",
        "# Train the MLP using 80% training set\n",
        "mlpClassifier.fit(X_train, y_train)\n",
        "# MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
        "# beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "# hidden_layer_sizes=(25, 25), learning_rate='constant',\n",
        "# learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
        "# n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
        "# random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
        "# validation_fraction=0.1, verbose=False, warm_start=False)\n",
        "\n",
        "score = mlpClassifier.score(X_test, y_test)\n",
        "# The classification score accuracy obtained.\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(178, 13)\n",
            "(178,)\n",
            "0.9444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snhR8AsEDp59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IRIS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PHwnHszE6Jz",
        "colab_type": "code",
        "outputId": "f26dfb2e-c14f-46c8-ace4-8d929d679547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "# Load the dataset from sklearn\n",
        "from sklearn.datasets import load_wine\n",
        "# Use the MLP class defined in sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# When return_X_y = True, the load function\n",
        "# return data and target instead of Bunch object.\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "\n",
        "\n",
        "print(type(X))\n",
        "# <class 'numpy.ndarray'>\n",
        "print(X.shape)\n",
        "# (178, 13)\n",
        "print(y.shape)\n",
        "# (178,)\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "stratify=y, random_state=42)\n",
        "# Create an object (model) for MLP.\n",
        "mlpClassifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "hidden_layer_sizes=(25, 25), random_state=1)\n",
        "# Init signature: MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
        "#solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant',\n",
        "#learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,\n",
        "#random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
        "#nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1,\n",
        "#beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "# Docstring:\n",
        "# Multi-layer Perceptron classifier.\n",
        "# This model optimizes the log-loss function using LBFGS or stochastic\n",
        "# gradient descent.\n",
        "# Parameters\n",
        "# ----------\n",
        "# hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
        "# The ith element represents the number of neurons in the ith\n",
        "# hidden layer.\n",
        "\n",
        "\n",
        "\n",
        "# solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
        "# The solver for weight optimization.\n",
        "# - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
        "# - 'sgd' refers to stochastic gradient descent.\n",
        "# - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
        "# by Kingma, Diederik, and Jimmy Ba\n",
        "# Note: The default solver 'adam' works pretty well on relatively\n",
        "# large datasets (with thousands of training samples or more) in terms of\n",
        "# both training time and validation score.\n",
        "# For small datasets, however, 'lbfgs' can converge faster and perform\n",
        "# better.\n",
        "# alpha : float, optional, default 0.0001\n",
        "# L2 penalty (regularization term) parameter.\n",
        "\n",
        "# Train the MLP using 80% training set\n",
        "mlpClassifier.fit(X_train, y_train)\n",
        "# MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
        "# beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "# hidden_layer_sizes=(25, 25), learning_rate='constant',\n",
        "# learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
        "# n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
        "# random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
        "# validation_fraction=0.1, verbose=False, warm_start=False)\n",
        "\n",
        "score = mlpClassifier.score(X_test, y_test)\n",
        "# The classification score accuracy obtained.\n",
        "print(score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(178, 13)\n",
            "(178,)\n",
            "0.9444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMYBhRlLFZsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}